# TL;DR

Admin VM in CSOC account that is associated with a particular child AWS account.

## 1. QuickStart

```
gen3 workon csoc <commons account>
```

## 2. Table of Contents 

- [1. QuickStart](#1-quickstart)
- [2. Overview](#2-overview)
- [3. Centralized  logging](#3-centralized-logging)
  - [3.1 How it works](#21-how-it-works)


## 2. Overview

The CSOC account is conceptualized to represent a centralized magement account for all commoms account. The commonds account, conceptually, would act like child accounts for CSOC.


**TODO** Add diagram that explain VPC peering.


## 3. Centralized logging

For each commons account, every VM living within it, is supposed to send logs to CloudWatchLogs for later analisys, security, control, etc. 

This CSOC account will also collect all those logs generated by its child accounts, for a more easy way to browse them all if required without needing to log into each account.

Additionally, logs are also sent to ElasticSearch and S3 simultaneously. ElasticSearch offers Kibana as part of the service, which makes it super easy to filter/query/anilize all those logs through indices.

![image of workflow](workflow.png)

[Figure 1] Image of workflow reflects how the logs are sent from different account into this CSOC account and how the process is. 


### 3.1 How it works

When logs come in through the data stream service it comes base64 encoded and zipped. We need to process it before sending it to ElasticSearch, otherwise it won't be properly interpretated and ultimately won't be inserted and create an index. 

An example how it is received:

```
{
  "Records": [
    {
      "eventID": "shardId-000000000000:49545115243490985018280067714973144582180062593244200961",
      "eventVersion": "1.0",
      "kinesis": {
        "approximateArrivalTimestamp": 1428537600,
        "partitionKey": "partitionKey-3",
        "data": "SGVsbG8sIHRoaXMgaXMgYSB0ZXN0IDEyMy4=",
        "kinesisSchemaVersion": "1.0",
        "sequenceNumber": "49545115243490985018280067714973144582180062593244200961"
      },
      "invokeIdentityArn": "arn:aws:iam::EXAMPLE",
      "eventName": "aws:kinesis:record",
      "eventSourceARN": "arn:aws:kinesis:EXAMPLE",
      "eventSource": "aws:kinesis",
      "awsRegion": "us-east-1"
    }
  ]
}
```
[Table 1] Example fo data received through streaming service

The actual data from child accountd is in the `data` part of the above JSON, the rest is added by the service which is also nice to have. The lamda function takes care of decoding the data and make it nice for ElasticSearch.

Logs are being sent to S3 as well. There are multiple ways to store the data into S3, as it comes from the stream, or processed the same way we do for ElasticSearch. As for now, we are processing it so we don't need to decode/uncompress if we want/need to see the content.
